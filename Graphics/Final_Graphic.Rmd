---
title: "Producing Graphics For Portland Sediemtn Toxicity Data"
output: html_notebook
---

# Install Libraries
```{r}
library(readxl)
library(tidyverse)
library(GGally)
library(maxLik)
```

# Source Functions for Non-detects
```{r}
sibfldnm <- 'Analysis'
parent <- dirname(getwd())
sibling <- paste(parent,sibfldnm, sep = '/')
fn <- 'LNConditionalMeans.R'

source(paste(sibling,fn, sep='/'))
```

# Create Site Data
```{r}
sitename <-  c("East End Beach", "Amethyst Lot",
               "Maine State Pier/Ocean Gateway", "Maine Wharf",
               "Portland Pier", "Chandler's Wharf",
               "Union Wharf", "Union Wharf", "Wright Wharf",
               "Holyoke Wharf", "Deake's Wharf",
               "Portland Yacht Services", "Ricker's Wharf",
               "South Port Marina", "Aspasia/Sunset Marina",
               "Port Harbor/ Breakwater Marina")
  sitecode <- c('CSP-1', 'CSP-2', 'CSP-3', 'CSP-4', 'CSP-5',
                'CSP-6', 'CSP-7', 'CSP-7D', 'CSP-8', 'CSP-9',
                'CSP-10', 'CSP-11', 'CSP-12', 'CSS-13', 'CSP-14',
                'CSS-15')

site.info <- tibble(sitecode,sitename)
head(site.info)
rm(sitecode, sitename)
```

up process just to generate a list of parameters. So, we generate this by hand.
# ListS of Names of Parameters
```{r}
sibfldnm <- 'Derived Data'
parent <- dirname(getwd())
sibling <- paste(parent,sibfldnm, sep = '/')
fn <- 'working.data.xls'

sed_names <- c('% COARSE SAND', '% FINE SAND', '% MEDIUM SAND',
                'FINES', 'GRAVEL', 'MOISTURE', 'SOLIDS, TOTAL',
                'TOTAL ORGANIC CARBON (REP1)', 'TOTAL ORGANIC CARBON (REP2)')

metal_names <- read_excel(paste(sibling,fn, sep='/'),
                           sheet = "Metals", skip = 3) %>%
  select(1) %>%
  slice(1:8) %>%
  mutate(PARAMETER_NAME = substr(PARAMETER_NAME, 1, nchar(PARAMETER_NAME)-7))
metal_names <- metal_names$PARAMETER_NAME

pah_names <- read_excel(paste(sibling,fn, sep='/'),
                        sheet = "PAHs", skip = 3) %>%
  select(1) %>%
  slice(1:16)
pah_names <- pah_names$PARAMETER_NAME

pcb_names <- read_excel(paste(sibling,fn, sep='/'),
                        sheet = "PCBs", skip = 3) %>%
  select(1) %>%
  slice(1:22) #%>%
pcb_names <- pcb_names$PARAMETER_NAME

pesticide_names <- read_excel(paste(sibling,fn, sep='/'),
                               sheet = "Pesticides", skip = 3) %>%
  select(1) %>%
  slice(1:8)
pesticide_names <- pesticide_names$PARAMETER_NAME

(parmnames = c(sed_names, metal_names, pah_names, pcb_names, pesticide_names))

```

# Load Basic Data
```{r}
sibfldnm <- 'Derived Data'
parent <- dirname(getwd())
sibling <- paste(parent,sibfldnm, sep = '/')
fn <- 'working.data.xls'

the.data <- read_excel(paste(sibling,fn, sep='/'), 
    sheet = "Combined", col_types = c("skip", 
        "text", "skip", "skip", "skip", 
        "skip", "skip", "skip", "skip", 
        "text", "text", "numeric", "text", 
        "numeric", "text", "numeric", "numeric", 
        "text", "text", "text", "skip", 
        "skip", "skip", "skip", "skip", 
        "numeric", "numeric", "skip", "skip", 
        "skip", "skip", "skip", "skip", 
        "skip")) %>%
  mutate(SAMPLE_ID = factor(SAMPLE_ID, levels = c("CSP-1", "CSP-2", "CSP-3",
                                                  "CSP-4", "CSP-5", "CSP-6",
                                                  "CSP-7", "CSP-7D", "CSP-8",
                                                  "CSP-9", "CSP-10", "CSP-11",
                                                  "CSP-12", "CSS-13", "CSP-14",
                                                  "CSS-15"))) %>%
  # Remove ', TOTAL' from the end of the names of metals
  mutate(PARAMETER_NAME = 
           ifelse(substr(PARAMETER_NAME, nchar(PARAMETER_NAME)-6,
                          nchar(PARAMETER_NAME)) == ', TOTAL',
                  substr(PARAMETER_NAME, 1, nchar(PARAMETER_NAME)-7),
                  PARAMETER_NAME))
```

# Assemble Simplified Data
We filter data to eliminate QA/QC samples, then combine Reporting Limits and Observed COncentrations into data on concentrations and a flag indicating which are censored values.
```{r}
sed_data_long <- the.data %>% 
  filter (the.data$PARAMETER_NAME %in% parmnames) %>%
  filter(is.na(`%_RECOVERY`)) %>%
  filter(SAMPLE_ID != 'QC') %>%
  mutate(CONCENTRATION = ifelse(is.na(CONCENTRATION) & LAB_QUALIFIER %in% c('U', 'J'),
                                REPORTING_LIMIT, CONCENTRATION)) %>%
  group_by(SAMPLE_ID, PARAMETER_NAME) %>%
  summarize(CONCENTRATION = mean(CONCENTRATION, na.rm=TRUE),
            samples = n(),
            censored = sum(LAB_QUALIFIER=='U', na.rm=TRUE)) %>%
  ungroup() %>%
  rename(Contaminant = PARAMETER_NAME) %>%
  mutate(Contaminant = factor(Contaminant)) %>%
  
  #Create group variable to facilitate subsetting.
  mutate(cgroup = ifelse(Contaminant %in% sed_names,1,
                   ifelse(Contaminant %in% metal_names,2,
                      ifelse(Contaminant %in% pah_names,3,
                          ifelse(Contaminant %in% pcb_names,4,
                              ifelse(Contaminant %in% pesticide_names,
                                     5,0)))))) %>%
  mutate(cgroup = factor(cgroup, labels = c('Sediment', 'Metals', 'PAHs',
                                         'PCBs', 'Pesticides')))  %>% 
  mutate(Contaminant = fct_reorder(Contaminant, as.numeric(Contaminant))) %>%
  mutate(Contaminant = fct_reorder(Contaminant, as.numeric(cgroup)))

levels(sed_data_long$Contaminant)
```
## A Check for Half Censored Observations
Since in a few cases, we averaged two samples togeter, it is possible that we have averaged a non-detect with a detection.  It turns out we did, but just once.
```{r}
sum(sed_data_long$censored> 0 & sed_data_long$censored<sed_data_long$samples)

sed_data_long[which(sed_data_long$censored> 0 & sed_data_long$censored<sed_data_long$samples),]

the.data[the.data$SAMPLE_ID=='CSS-15' & the.data$PARAMETER_NAME=="4,4'-DDT",]
```
Note that these two DDT results were flagged as problematic because differences between the two observations were greater than an acceptable relative percent difference. That does not engender great confidence in the data....

We have to pick this up again later, when we calculate values for Pesticide Totals.

# Load the Screening Value Data
THis may or may not work.  Mismatches in facto levels cause messy problems wit hgraphics, and I'm not sure I need all the details
```{r}
sibfldnm <- 'Derived Data'
parent <- dirname(getwd())
sibling <- paste(parent,sibfldnm, sep = '/')
fn <- 'Marine Sediment Screening Values simplified.xlsx'

SQUIRTS <- read_excel(paste(sibling,fn, sep='/')) %>%
   select(1:8) %>%
   #filter(Chemical %in% levels(sed_data_long$Contaminant)) %>%
   filter(! is.na(Chemical)) %>%
   mutate(Chemical = factor(Chemical, levels = levels(sed_data_long$Contaminant)))
  
levels(SQUIRTS$Chemical)
```

# Strategy
Overall, we want a viually simple graphic that captures the most important aspects of contamination in Portland harbor.  From other analyses, we know that 
1.  Metals levels are cosnstently below screaning levels
2. Most pesticide residues were never or almost never detected, teh only exceptions being the DDT residues.
3.  Other contaminants exceed screening levels at about half of sampling locations.
4. Levels of contaminants are highly correlated

Multiple contaminants with complicated names can make for intimidating graphic.  We need to identify ways to simplify the data display without distorting the meaning.  Screening levels are available for Total PAHs, Total PCBs, and sum of DDT residues, suggesting we can focus on those three aggregate indicators of contamination and thus simplify data presentation.  Ther is no equivalnet way of simplifying display of hte metals data, but all metals are below levels of concern.

The data is complicated in several ways:
1. Many non-detects
2. Elevated reporting limits for organic contaminants from sample CSP-8.  For PCBs the reporting limit is sufficiently high to bias any approach to estimating PSB loads. incorporating  for PCBs.


# Metals Data
No Reorganization Needed.  WE could show one or two metals of general interest -- like Mercury -- or just report that all are below levels of concern.

# PAH Data

# PCB Data
```{r}
pcb.data.long<- sed_data_long %>%
  filter (cgrooup='PCBs') %>%
  
  # CSP-8 had exceptionally high PCB detection limits
  filter(SAMPLE_ID != 'CSP-8') %>% 
  
  muteate(censored = censored>0)


pcbres <- pcb.data.long %>%
  group_by(PARAMETER_NAME) %>%
  mutate(LikCensored = LNConditionalMeans(CONCENTRATION, censored)) %>%
  ungroup()  %>%
  mutate(HalfDL = ifelse(censored, CONCENTRATION/2, CONCENTRATION)) %>%
  group_by(SAMPLE_ID) %>%
  summarize(LNtotPCB = sum(LikCensored),
            halfDLtotPCB = sum(HalfDL),
            totPCB = sum(CONCENTRATION))
```

# Pesticides Data
We want to report Total DDT Residues.  Other pesticides were observed too rarely to be worth reporting on.
```{r}
Pesticides.data.long <- the.data %>%
  filter (the.data$PARAMETER_NAME %in%  c(`4,4'-DDD`, `4,4'-DDE`, `4,4'-DDT`)) %>%
  filter(is.na(`%_RECOVERY`)) %>%
  filter(SAMPLE_ID != 'QC') %>%
  mutate(CONCENTRATION = ifelse(is.na(CONCENTRATION) & LAB_QUALIFIER %in% c('U', 'J'),
                                REPORTING_LIMIT, CONCENTRATION)) %>%
  group_by(SAMPLE_ID, PARAMETER_NAME) %>%
  summarize(CONCENTRATION = mean(CONCENTRATION, na.rm=TRUE),
            samples = n(),
            censored = sum(LAB_QUALIFIER=='U', na.rm=TRUE)) %>%
  ungroup()
```



#Correcting for Half non-detects
When we average across the two values, we are averaging a non-detect with a significantly higher observation. In the preceeding analysis,  we flagged the average as a censored value, below the average.  That may bias the value for certain analyses.


In our preliminary analyses, we saw that pesticide levels for CSS-15 were relatively low. If you replace the ND with the "detection limit" -- which here would be an average of the observed concentration and the DL -- that value slightly exceeds ERL.  Using either half the DL or maximum likelihood estimation, the sample lies below ERL.

So. how we handle this split sample matters. 

To be thorough, we need to calculate our censored estimates (ND, Half ND and Maximum Likelihood) on the original raw data and average the results, rather than just assume everything works out o.k. when applied to the avergaged value.

Obviously,if we are looking at the full detection limit, the average of a sum is the sum of the averages, and it makes no difference, but it should matter for the other two estimators, especially for the maximum liklihood estimator.

ML estimator on the non-detect and average that result with the detected value, rather than applying the ML estimator to teh averaged value.  I can't imagine it will make much difference, but....

### Calculate half DL limits both ways
```{r}
tmp <- the.data %>%
  filter(PARAMETER_NAME=="4,4'-DDT") %>%
  filter(SAMPLE_ID == 'CSS-15') %>%
  filter(is.na(`%_RECOVERY`)) %>%
  filter(SAMPLE_ID != 'QC') %>%
  mutate(censored = LAB_QUALIFIER %in% c('U', 'J')) %>%
  mutate(CONCENTRATION = ifelse(censored, REPORTING_LIMIT, CONCENTRATION)) %>%
  select(censored, CONCENTRATION, REPORTING_LIMIT) %>%
  mutate(halfdl = ifelse(censored, REPORTING_LIMIT, CONCENTRATION))
mean(tmp%>%pull(CONCENTRATION))/2
mean(tmp%>%pull(halfdl))
```
So, doing this right sharply increases our estimate.

### Calculate Maximum Likelihood Estimator two ways
First the correct way, by averaging the two estimates
```{r}
est <- the.data %>%
  filter(PARAMETER_NAME=="4,4'-DDT") %>%
  filter(is.na(`%_RECOVERY`)) %>%
  filter(SAMPLE_ID != 'QC') %>%
  select(SAMPLE_ID, CONCENTRATION, REPORTING_LIMIT, LAB_QUALIFIER) %>%
  mutate(censored = LAB_QUALIFIER %in% c('U', 'J')) %>%
  mutate(CONCENTRATION = ifelse(censored, REPORTING_LIMIT, CONCENTRATION)) %>%
  mutate(lnest = LNConditionalMeans(CONCENTRATION, censored)) %>%
  filter(SAMPLE_ID == 'CSS-15') %>%
  pull(lnest)
est
mean(est)
```
Second, the simple way, by calculating estimates based on average value.
```{r}
sed_data_long %>%
  filter(Contaminant=="4,4'-DDT") %>%
  mutate(censored = censored>0) %>%
  mutate(lnest = LNConditionalMeans(CONCENTRATION, censored)) %>%
  filter(SAMPLE_ID == 'CSS-15') %>%
  pull(lnest)
```
So, this DOES make a difference for our ML estimate, as suspected..


